{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2123960b-13dd-4ffa-ad5d-011e2a6b9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# 모든 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf98ed5-cf49-4832-9cfd-6162eb1164f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/HDD1/bbq9088/miniconda3/envs/molberta/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from deepchem.feat.molecule_featurizers import CircularFingerprint\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Crippen, Lipinski, Fragments, rdMolDescriptors\n",
    "\n",
    "# 로드된 Roberta 모델과 Tokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484aaf49-e7c4-43ad-85ae-ae02c13acf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a3a392-97ca-474f-8a27-7810d4605e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./origin_model/roberta were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 로드한 RoBERTa 모델\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./origin_model/roberta/tokenizer_folder\")\n",
    "model = RobertaModel.from_pretrained(\"./origin_model/roberta\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2df1c4-74d6-444b-921a-7a03c45be353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizer 설정 (CircularFingerprint 사용)\n",
    "featurizer = CircularFingerprint(radius=2, size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c10d14-312a-4f3b-b824-5112165870c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 (ClinTox, SIDER, Tox21)\n",
    "tasks_clintox, datasets_clintox, transformers_clintox = dc.molnet.load_clintox(featurizer=featurizer, splitter=None, transformers=[], reload=True)\n",
    "dataset_clintox = datasets_clintox[0]\n",
    "df_clintox = pd.DataFrame({'smiles': dataset_clintox.ids, 'FDA_APPROVED': dataset_clintox.y[:, 0], 'CT_TOX': dataset_clintox.y[:, 1]}).dropna()\n",
    "\n",
    "tasks_sider, datasets_sider, transformers_sider = dc.molnet.load_sider(featurizer=featurizer, splitter=None, transformers=[], reload=True)\n",
    "dataset_sider = datasets_sider[0]\n",
    "df_sider = pd.DataFrame(data=dataset_sider.y, columns=tasks_sider)\n",
    "df_sider['smiles'] = dataset_sider.ids\n",
    "df_sider = df_sider.dropna()\n",
    "\n",
    "tasks_tox21, datasets_tox21, transformers_tox21 = dc.molnet.load_tox21(featurizer=featurizer, splitter=None, transformers=[], reload=True)\n",
    "dataset_tox21 = datasets_tox21[0]\n",
    "df_tox21 = pd.DataFrame(data=dataset_tox21.y, columns=tasks_tox21)\n",
    "df_tox21['smiles'] = dataset_tox21.ids\n",
    "df_tox21 = df_tox21.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d422c48b-7892-4832-9867-9c712c2ead1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES에 대한 입력 문장 생성 함수\n",
    "#def create_input_text(smiles):\n",
    "#    return f\"SMILES: {smiles}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9933db-2e01-435c-8d6b-904a7d749d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES에 대한 분자 특성 계산 함수\n",
    "def calculate_properties(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    properties = []\n",
    "    try:\n",
    "        properties.append(Descriptors.MolWt(mol) if mol else None)\n",
    "        properties.append(Crippen.MolLogP(mol) if mol else None)\n",
    "        properties.append(Descriptors.TPSA(mol) if mol else None)\n",
    "        properties.append(Lipinski.NumHAcceptors(mol) if mol else None)\n",
    "        properties.append(Lipinski.NumHDonors(mol) if mol else None)\n",
    "        properties.append(Lipinski.NumRotatableBonds(mol) if mol else None)\n",
    "        properties.append(Chem.GetFormalCharge(mol) if mol else None)\n",
    "        properties.append(rdMolDescriptors.CalcNumAtomStereoCenters(mol) if mol else None)\n",
    "        properties.append(rdMolDescriptors.CalcFractionCSP3(mol) if mol else None)\n",
    "        properties.append(Descriptors.NumAliphaticCarbocycles(mol) if mol else None)\n",
    "        properties.append(Descriptors.NumAromaticRings(mol) if mol else None)\n",
    "        properties.append(Descriptors.NumHeteroatoms(mol) if mol else None)\n",
    "        properties.append(Fragments.fr_COO(mol) if mol else None)\n",
    "        properties.append(Fragments.fr_Al_OH(mol) if mol else None)\n",
    "        properties.append(Fragments.fr_alkyl_halide(mol) if mol else None)\n",
    "        properties.append(Descriptors.NumAromaticCarbocycles(mol) if mol else None)\n",
    "        properties.append(Fragments.fr_piperdine(mol) if mol else None)\n",
    "        properties.append(Fragments.fr_methoxy(mol) if mol else None)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate properties for SMILES: {smiles}. Error: {e}\")\n",
    "        return [None] * 18  # 오류 발생 시 모든 값을 None으로 반환\n",
    "    \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57041283-61be-436a-ab00-ca7da366d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_text(smiles):\n",
    "    properties = calculate_properties(smiles)\n",
    "    \n",
    "    property_names = [\n",
    "        \"Molecular Weight\", \"LogP\", \"Topological Polar Surface Area\", \n",
    "        \"Number of Hydrogen Bond Acceptors\", \"Number of Hydrogen Bond Donors\", \n",
    "        \"Number of Rotatable Bonds\", \"Formal Charge\", \"Number of Atom Stereocenters\", \n",
    "        \"Fraction of sp3 Carbon Atoms\", \"Number of Aliphatic Carbocycles\", \n",
    "        \"Number of Aromatic Rings\", \"Number of Heteroatoms\", \"Number of Carboxylic Acid Groups\", \n",
    "        \"Number of Aliphatic Alcohol Groups\", \"Number of Alkyl Halide Groups\", \n",
    "        \"Number of Aromatic Carbocycles\", \"Number of Piperidine Groups\", \n",
    "        \"Number of Methoxy Groups\"\n",
    "    ]\n",
    "    \n",
    "    # None 값이 아닌 속성만 포함\n",
    "    properties_text = \" | \".join(\n",
    "        [f\"{name}: {value:.5f}\" for name, value in zip(property_names, properties) if value is not None]\n",
    "    )\n",
    "    \n",
    "    input_text = f\"SMILES: {smiles} | {properties_text}\" if properties_text else f\"SMILES: {smiles}\"\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b8b166-62c7-4a6d-a9d2-22cef79c0fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 추출 함수 (RoBERTa 기반)\n",
    "def get_embeddings(model, tokenizer, smiles_list):\n",
    "    embeddings = []\n",
    "\n",
    "    for smiles in tqdm(smiles_list, desc=\"Processing SMILES with Roberta\"):\n",
    "        input_text = create_input_text(smiles)\n",
    "        \n",
    "        # 입력 텍스트를 토큰화\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "        # 모델에 입력하여 임베딩 추출\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24769463-0f11-43c6-bf0c-3a5a104031a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-겹 교차 검증을 사용한 다중 레이블/다중 출력 모델 학습 및 평가\n",
    "def train_and_evaluate_kfold(X, y, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    f1_micro_scores = []\n",
    "    f1_macro_scores = []\n",
    "    auc_roc_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = MultiOutputClassifier(LogisticRegression(max_iter=500, solver='lbfgs', penalty='l2'))\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = np.array([estimator.predict_proba(X_test)[:, 1] for estimator in model.estimators_]).T\n",
    "        \n",
    "        # Calculate F1-scores\n",
    "        f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "        f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "        f1_micro_scores.append(f1_micro)\n",
    "        f1_macro_scores.append(f1_macro)\n",
    "        \n",
    "        # Calculate AUC-ROC score for each label and compute the mean\n",
    "        aucrocs = []\n",
    "        for i in range(y_test.shape[1]):\n",
    "            if len(np.unique(y_test[:, i])) > 1:  # Only if both classes are present\n",
    "                aucrocs.append(roc_auc_score(y_test[:, i], y_pred_proba[:, i]))\n",
    "        auc_roc_mean = np.mean(aucrocs) if aucrocs else float('nan')\n",
    "        auc_roc_scores.append(auc_roc_mean)\n",
    "\n",
    "    f1_micro_mean = np.mean(f1_micro_scores)\n",
    "    f1_macro_mean = np.mean(f1_macro_scores)\n",
    "    auc_roc_mean_overall = np.nanmean(auc_roc_scores)  # Handle NaN values gracefully\n",
    "    f1_micro_std = np.std(f1_micro_scores)\n",
    "    f1_macro_std = np.std(f1_macro_scores)\n",
    "    auc_roc_std = np.nanstd(auc_roc_scores)\n",
    "\n",
    "    print(f\"Average F1-score (Micro): {f1_micro_mean:.4f} ± {f1_micro_std:.4f}\")\n",
    "    print(f\"Average F1-score (Macro): {f1_macro_mean:.4f} ± {f1_macro_std:.4f}\")\n",
    "    print(f\"Average AUC-ROC score: {auc_roc_mean_overall:.4f} ± {auc_roc_std:.4f}\")\n",
    "\n",
    "    return f1_micro_scores, f1_macro_scores, auc_roc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a17b9f53-a10d-4be6-af76-00cca1a71e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋별 임베딩 생성 및 학습/평가\n",
    "embedding_dir = os.path.join(os.getcwd(), 'Embedding')\n",
    "result_dir = os.path.join(os.getcwd(), 'Results')\n",
    "os.makedirs(embedding_dir, exist_ok=True)\n",
    "os.makedirs(result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68635792-6888-41e5-bf6c-49e41bde75a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing ClinTox ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361f6a6c0fef4818956b84e034eedc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing SMILES with Roberta:   0%|          | 0/1480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training and evaluating on ClinTox ===\n",
      "Average F1-score (Micro): 0.9776 ± 0.0090\n",
      "Average F1-score (Macro): 0.9118 ± 0.0306\n",
      "Average AUC-ROC score: 0.9790 ± 0.0243\n",
      "ClinTox - F1-score (Micro): 0.9776 ± 0.0090\n",
      "ClinTox - F1-score (Macro): 0.9118 ± 0.0306\n",
      "ClinTox - AUC-ROC: 0.9790 ± 0.0243\n",
      "Metrics for ClinTox saved to /HDD1/bbq9088/GPT-MolBERTa/model_predict/Results/ClinTox_metrics.txt\n",
      "\n",
      "=== Processing SIDER ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456f2bf4f2b04eba894d4997b9b85378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing SMILES with Roberta:   0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:40:36] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:36] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:40:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training and evaluating on SIDER ===\n",
      "Average F1-score (Micro): 0.7942 ± 0.0058\n",
      "Average F1-score (Macro): 0.6065 ± 0.0048\n",
      "Average AUC-ROC score: 0.6178 ± 0.0088\n",
      "SIDER - F1-score (Micro): 0.7942 ± 0.0058\n",
      "SIDER - F1-score (Macro): 0.6065 ± 0.0048\n",
      "SIDER - AUC-ROC: 0.6178 ± 0.0088\n",
      "Metrics for SIDER saved to /HDD1/bbq9088/GPT-MolBERTa/model_predict/Results/SIDER_metrics.txt\n",
      "\n",
      "=== Processing Tox21 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3e527db9f743e6b98cb07b05a1ca83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing SMILES with Roberta:   0%|          | 0/7823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:40:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training and evaluating on Tox21 ===\n",
      "Average F1-score (Micro): 0.1342 ± 0.0212\n",
      "Average F1-score (Macro): 0.1280 ± 0.0195\n",
      "Average AUC-ROC score: 0.7503 ± 0.0085\n",
      "Tox21 - F1-score (Micro): 0.1342 ± 0.0212\n",
      "Tox21 - F1-score (Macro): 0.1280 ± 0.0195\n",
      "Tox21 - AUC-ROC: 0.7503 ± 0.0085\n",
      "Metrics for Tox21 saved to /HDD1/bbq9088/GPT-MolBERTa/model_predict/Results/Tox21_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, df in [(\"ClinTox\", df_clintox), (\"SIDER\", df_sider), (\"Tox21\", df_tox21)]:\n",
    "    print(f\"\\n=== Processing {dataset_name} ===\")\n",
    "\n",
    "    # 임베딩 생성\n",
    "    embeddings = get_embeddings(model, tokenizer, df['smiles'].tolist())\n",
    "\n",
    "    # 레이블 준비\n",
    "    labels = df.drop(columns=['smiles']).values\n",
    "\n",
    "    # K-겹 교차 검증을 통한 학습 및 평가\n",
    "    print(f\"=== Training and evaluating on {dataset_name} ===\")\n",
    "    f1_micro_scores, f1_macro_scores, auc_roc_scores = train_and_evaluate_kfold(embeddings, labels)\n",
    "\n",
    "    # 결과 요약\n",
    "    f1_micro_mean = np.mean(f1_micro_scores)\n",
    "    f1_macro_mean = np.mean(f1_macro_scores)\n",
    "    auc_roc_mean = np.nanmean(auc_roc_scores)  # NaN 처리\n",
    "\n",
    "    f1_micro_std = np.std(f1_micro_scores)\n",
    "    f1_macro_std = np.std(f1_macro_scores)\n",
    "    auc_roc_std = np.nanstd(auc_roc_scores)\n",
    "\n",
    "    print(f\"{dataset_name} - F1-score (Micro): {f1_micro_mean:.4f} ± {f1_micro_std:.4f}\")\n",
    "    print(f\"{dataset_name} - F1-score (Macro): {f1_macro_mean:.4f} ± {f1_macro_std:.4f}\")\n",
    "    print(f\"{dataset_name} - AUC-ROC: {auc_roc_mean:.4f} ± {auc_roc_std:.4f}\")\n",
    "\n",
    "    # 결과 저장\n",
    "    result_dir = os.path.join(os.getcwd(), 'Results')\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    result_file = os.path.join(result_dir, f\"{dataset_name}_metrics.txt\")\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "        f.write(f\"F1-score (Micro): {f1_micro_mean:.4f} ± {f1_micro_std:.4f}\\n\")\n",
    "        f.write(f\"F1-score (Macro): {f1_macro_mean:.4f} ± {f1_macro_std:.4f}\\n\")\n",
    "        f.write(f\"AUC-ROC: {auc_roc_mean:.4f} ± {auc_roc_std:.4f}\\n\")\n",
    "\n",
    "    print(f\"Metrics for {dataset_name} saved to {result_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
