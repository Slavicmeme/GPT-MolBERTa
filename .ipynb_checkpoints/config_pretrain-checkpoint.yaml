
model: roberta # bert, roberta

batch_size: 30
epochs: 7

model_bert:
  vocab_size: 50000
  hidden_size: 768
  num_hidden_layers: 6
  num_attention_heads: 12
  max_position_embeddings: 512

model_roberta:
  vocab_size: 50000
  hidden_size: 768
  num_hidden_layers: 6
  num_attention_heads: 12
  max_position_embeddings: 514
