
dataframe: freesolv # clintox, sider, tox21, freesolv, lipo, esol

dataset:
  batch_size: 4
  num_workers: 0
  valid_size: 0.1
  test_size: 0.1
  task: regression # classification, regression
  splitting: scaffold # random, scaffold

model: roberta # bert, roberta
pretrain: pretraining # pretraining, scratch
early_stopping: 8 # Stop training if there is no improvement in model performance after these number of epochs

model_bert:
  vocab_size: 50000
  hidden_size: 768
  num_hidden_layers: 6
  num_attention_heads: 12
  max_position_embeddings: 512

model_roberta:
  vocab_size: 50000
  hidden_size: 768
  num_hidden_layers: 6
  num_attention_heads: 12
  max_position_embeddings: 514

device: cuda:0 # cuda:0, cpu

max_length: 512

learning_rate: 1e-5
epoch: 30
